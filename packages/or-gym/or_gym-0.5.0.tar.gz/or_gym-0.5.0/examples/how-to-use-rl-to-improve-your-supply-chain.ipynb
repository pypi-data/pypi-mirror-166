{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "937597e4",
   "metadata": {},
   "source": [
    "# How to Use Deep Reinforcement Learning to Improve your Supply Chain\n",
    "\n",
    "Full write up available [here](https://www.datahubbs.com/how-to-use-deep-reinforcement-learning-to-improve-your-supply-chain/).\n",
    "\n",
    "Note Ray is not a dependency of OR-Gym. We want OR-Gym to be able to stand independently of other RL libraries as much as possible.\n",
    "\n",
    "There have been breaking changes that have been introduced in later version of Ray which affect this environment in particular. To ensure no conflicts, please run:\n",
    "- `pip install ray==1.0.0`\n",
    "- `pip install ray[rllib]`\n",
    "- `pip install ray[tune]`\n",
    "- `pip install tensorflow==2.3.0`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fefefc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/christian/anaconda3/envs/or-gym-dev/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import or_gym\n",
    "from or_gym.utils import create_env\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40fa580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_env(env_name, env_config={}):\n",
    "    env = create_env(env_name)\n",
    "    tune.register_env(env_name, \n",
    "        lambda env_name: env(env_name,\n",
    "            env_config=env_config))\n",
    "\n",
    "# Environment and RL Configuration Settings\n",
    "env_name = 'InvManagement-v1'\n",
    "# env_name = \"Knapsack-v0\"\n",
    "env_config = {} # Change environment parameters here\n",
    "rl_config = dict(\n",
    "    env=env_name,\n",
    "    num_workers=2,\n",
    "    env_config=env_config,\n",
    "    model=dict(\n",
    "        vf_share_layers=False,\n",
    "        fcnet_activation='elu',\n",
    "        fcnet_hiddens=[256, 256]\n",
    "    ),\n",
    "    lr=1e-5\n",
    ")\n",
    " \n",
    "# Register environment\n",
    "register_env(env_name, env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea13304f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-02 10:53:41,358\tINFO services.py:1164 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2022-09-02 10:53:44,394\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2022-09-02 10:53:44,398\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=9662)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/envs/or-gym-dev/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9662)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9662)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9660)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/envs/or-gym-dev/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9660)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9660)\u001b[0m non-resource variables are not supported in the long term\n",
      "2022-09-02 10:54:04,675\tINFO trainable.py:252 -- Trainable.setup took 20.284 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-09-02 10:54:04,677\tWARNING util.py:39 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/christian/anaconda3/envs/or-gym-dev/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:872: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9662)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/envs/or-gym-dev/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:872: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9662)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9662)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=9660)\u001b[0m WARNING:tensorflow:From /home/christian/anaconda3/envs/or-gym-dev/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:872: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9660)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9660)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 155\tReward: 339.55"
     ]
    }
   ],
   "source": [
    "# Initialize Ray and Build Agent\n",
    "ray.init(ignore_reinit_error=True)\n",
    "agent = PPOTrainer(env=env_name,\n",
    "    config=rl_config)\n",
    " \n",
    "results = []\n",
    "for i in range(500):\n",
    "    res = agent.train()\n",
    "    results.append(res)\n",
    "    if (i+1) % 5 == 0:\n",
    "        print('\\rIter: {}\\tReward: {:.2f}'.format(\n",
    "                i+1, res['episode_reward_mean']), end='')\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e41cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(3,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unpack values from each iteration\n",
    "rewards = np.hstack([i['hist_stats']['episode_reward'] \n",
    "    for i in results])\n",
    "pol_loss = [\n",
    "    i['info']['learner']['default_policy']['policy_loss'] \n",
    "    for i in results]\n",
    "vf_loss = [\n",
    "    i['info']['learner']['default_policy']['vf_loss'] \n",
    "    for i in results]\n",
    "p = 100\n",
    "mean_rewards = np.array([np.mean(rewards[i-p:i+1]) \n",
    "                if i >= p else np.mean(rewards[:i+1]) \n",
    "                for i, _ in enumerate(rewards)])\n",
    "std_rewards = np.array([np.std(rewards[i-p:i+1])\n",
    "               if i >= p else np.std(rewards[:i+1])\n",
    "               for i, _ in enumerate(rewards)])\n",
    "fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "gs = fig.add_gridspec(2, 4)\n",
    "ax0 = fig.add_subplot(gs[:, :-2])\n",
    "ax0.fill_between(np.arange(len(mean_rewards)), \n",
    "                 mean_rewards - std_rewards, \n",
    "                 mean_rewards + std_rewards, \n",
    "                 label='Standard Deviation', alpha=0.3)\n",
    "ax0.plot(mean_rewards, label='Mean Rewards')\n",
    "ax0.set_ylabel('Rewards')\n",
    "ax0.set_xlabel('Episode')\n",
    "ax0.set_title('Training Rewards')\n",
    "ax0.legend()\n",
    "ax1 = fig.add_subplot(gs[0, 2:])\n",
    "ax1.plot(pol_loss)\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_title('Policy Loss')\n",
    "ax2 = fig.add_subplot(gs[1, 2:])\n",
    "ax2.plot(vf_loss)\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_title('Value Function Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e494fbe",
   "metadata": {},
   "source": [
    "# Derivative Free Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb7398",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m high_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mall(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow \u001b[39m==\u001b[39m low_)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mall(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh \u001b[39m==\u001b[39m high_)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741bbd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, array([100,  90,  80], dtype=int16), array([0, 0, 0], dtype=int16))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def base_stock_policy(policy, env):\n",
    "  '''\n",
    "  Implements a re-order up-to policy. This means that for\n",
    "  each node in the network, if the inventory at that node \n",
    "  falls below the level denoted by the policy, we will \n",
    "  re-order inventory to bring it to the policy level.\n",
    "  \n",
    "  For example, policy at a node is 10, current inventory\n",
    "  is 5: the action is to order 5 units.\n",
    "  '''\n",
    "  assert len(policy) == len(env.init_inv), (\n",
    "    'Policy should match number of nodes in network' + \n",
    "    '({}, {}).'.format(len(policy), len(env.init_inv)))\n",
    "  \n",
    "  # Get echelon inventory levels\n",
    "  if env.period == 0:\n",
    "    inv_ech = np.cumsum(env.I[env.period] +\n",
    "      env.T[env.period])\n",
    "  else:\n",
    "    inv_ech = np.cumsum(env.I[env.period] +\n",
    "      env.T[env.period] - env.B[env.period-1, :-1])\n",
    "      \n",
    "  # Get unconstrained actions\n",
    "  unc_actions = policy - inv_ech\n",
    "  unc_actions = np.where(unc_actions>0, unc_actions, 0)\n",
    "  \n",
    "  # Ensure that actions can be fulfilled by checking \n",
    "  # constraints\n",
    "  inv_const = np.hstack([env.I[env.period, 1:], np.Inf])\n",
    "  actions = np.minimum(env.c, np.minimum(unc_actions, inv_const))\n",
    "  return actions\n",
    "\n",
    "def dfo_func(policy, env, *args):\n",
    "    '''\n",
    "    Runs an episode based on current base-stock model \n",
    "    settings. This allows us to use our environment for the \n",
    "    DFO optimizer.\n",
    "    '''\n",
    "    env.reset() # Ensure env is fresh\n",
    "    rewards = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = base_stock_policy(policy, env)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    rewards = np.array(rewards)\n",
    "    prob = env.demand_dist.pmf(env.D, **env.dist_param)\n",
    "    \n",
    "    # Return negative of expected profit\n",
    "    return -1 / env.num_periods * np.sum(prob * rewards)\n",
    "  \n",
    "def optimize_inventory_policy(env_name, fun,\n",
    "  init_policy=None, env_config={}, method='Powell'):\n",
    "  \n",
    "  env = or_gym.make(env_name, env_config=env_config)\n",
    "  \n",
    "  if init_policy is None:\n",
    "      init_policy = np.ones(env.num_stages-1)\n",
    "      \n",
    "  # Optimize policy\n",
    "  out = minimize(fun=fun, x0=init_policy, args=env, \n",
    "      method=method)\n",
    "  policy = out.x.copy()\n",
    "  \n",
    "  # Policy must be positive integer\n",
    "  policy = np.round(np.maximum(policy, 0), 0).astype(int)\n",
    "  \n",
    "  return policy, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11da7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(3,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy, out = optimize_inventory_policy('InvManagement-v1',\n",
    "    dfo_func)\n",
    "print(\"Re-order levels: {}\".format(policy))\n",
    "print(\"DFO Info:\\n{}\".format(out))\n",
    "\n",
    "env = or_gym.make(env_name, env_config=env_config)\n",
    "eps = 1000\n",
    "rewards = []\n",
    "for i in range(eps):\n",
    "    env.reset()\n",
    "    reward = 0\n",
    "    while True:\n",
    "        action = base_stock_policy(policy, env)\n",
    "        s, r, done, _ = env.step(action)\n",
    "        reward += r\n",
    "        if done:\n",
    "            rewards.append(reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5147b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('or-gym-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc8a2230aa8b659650bd48bf6a546b4d453aa64d7078ee0770a23a54a48157c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
