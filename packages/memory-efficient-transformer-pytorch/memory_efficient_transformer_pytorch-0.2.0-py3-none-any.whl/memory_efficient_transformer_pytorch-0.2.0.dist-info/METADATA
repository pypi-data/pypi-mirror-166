Metadata-Version: 2.1
Name: memory-efficient-transformer-pytorch
Version: 0.2.0
Summary: Implementation on PyTorch of Self-attention Does Not Need $O(n^2)$ Memory
Home-page: https://github.com/yuta0306/memory-efficient-transformer-pytorch
License: MIT
Author: yuta0306
Author-email: you-2001-3-6@ezweb.ne.jp
Requires-Python: >=3.7,<3.11
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Requires-Dist: numpy (>=1.21.6)
Requires-Dist: torch (>=1.10.2)
Requires-Dist: tqdm (>=4.64.0)
Requires-Dist: transformers (>=4.20.1)
Project-URL: Repository, https://github.com/yuta0306/memory-efficient-transformer-pytorch
Description-Content-Type: text/markdown

# faster-transformer

*Self-attention Does Not Need $O(n^2)$ Memory*のPytorch実装

```bibtex
@misc{rabe2021selfattention,
    title={Self-attention Does Not Need $O(n^2)$ Memory}, 
    author={Markus N. Rabe and Charles Staats},
    year={2021},
    eprint={2112.05682},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
```

